{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ5E2SK46iFs"
      },
      "source": [
        "! pip install spektral\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/path_to_your_folder/')\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from spektral.layers import GATConv, GlobalAttentionPool\n",
        "\n",
        "import scipy.io as sio\n",
        "################################################################################\n",
        "# PARAMETERS\n",
        "################################################################################\n",
        "l2_reg = 5e-4            # Regularization rate for l2\n",
        "learning_rate = 1e-3     # Learning rate for Adam\n",
        "epochs = 20              # Number of training epochs\n",
        "batch_size = 32          # Batch size\n",
        "es_patience = 20         # Patience for early stopping\n",
        "K = 10 # Rician K factor\n",
        "N_list = [16, 32, 64]    # Number of RIS elements\n",
        "M_list = [16]            # Number of pilot symbols\n",
        "max_iter = 20\n",
        "################################################################################\n",
        "# LOAD DATA\n",
        "################################################################################\n",
        "\n",
        "\n",
        "for N in N_list:\n",
        "  for M in M_list:\n",
        "\n",
        "    NMSE_h = np.empty((max_iter, 21), dtype='float64')\n",
        "    NMSE_g = np.empty((max_iter, 21), dtype='float64')\n",
        "    NMSE_gh = np.empty((max_iter, 21), dtype='float64')\n",
        "\n",
        "    for iter in range(0, max_iter):\n",
        "      filename_train = '/content/drive/My Drive/path_to_your_folder/train_data/ch_est_data_' + 'M_' + str(M) + '_N_' + str(N) + '_K_10_snr_-30_0.mat'\n",
        "\n",
        "      data = sio.loadmat(filename_train)\n",
        "\n",
        "      A = data[\"A\"]\n",
        "      X = data[\"X\"]\n",
        "      E = data[\"E\"]\n",
        "      A = np.ones(A.shape)\n",
        "      y = data[\"y\"]\n",
        "\n",
        "      n_classes = y.shape[-1]  # Number of classes\n",
        "\n",
        "\n",
        "      # Parameters\n",
        "      P = X.shape[-2]       # Number of nodes in the graphs\n",
        "      F = X[0].shape[-1]    # Dimension of node features\n",
        "      S = E[0].shape[-1]    # Dimension of edge features\n",
        "      n_out = y.shape[-1]   # Dimension of the target\n",
        "\n",
        "      # Train/test split\n",
        "      A_train, A_test, \\\n",
        "      X_train, X_test, \\\n",
        "      E_train, E_test, \\\n",
        "      y_train, y_test = train_test_split(A, X, E, y, test_size=0.2, random_state=0)\n",
        "\n",
        "      ################################################################################\n",
        "      # BUILD MODEL\n",
        "      ################################################################################\n",
        "      X_in = Input(shape=(P, F))\n",
        "      A_in = Input(shape=(P, P))\n",
        "      E_in = Input(shape=(P, P, S))\n",
        "\n",
        "\n",
        "      gc1 = GATConv(128, activation='relu', kernel_regularizer=l2(l2_reg))([X_in, A_in])\n",
        "      gc2 = GATConv(32, activation='relu', kernel_regularizer=l2(l2_reg))([gc1, A_in])\n",
        "      pool = GlobalAttentionPool(128)(gc2)\n",
        "\n",
        "      output = Dense(n_classes)(pool)\n",
        "\n",
        "      # Build model\n",
        "      model = Model(inputs=[X_in, A_in], outputs=output)\n",
        "      optimizer = Adam(lr=learning_rate)\n",
        "      model.compile(optimizer=optimizer, loss='mse')\n",
        "      model.summary()\n",
        "\n",
        "      # Train model\n",
        "      estimator = model.fit([X_train, A_train],\n",
        "                y_train,\n",
        "                batch_size=batch_size,\n",
        "                validation_split=0.1,\n",
        "                epochs=epochs,\n",
        "                callbacks=[\n",
        "                    EarlyStopping(patience=es_patience, restore_best_weights=True)\n",
        "                ])\n",
        "\n",
        "      # Evaluate model\n",
        "      print('Evaluating model.')\n",
        "      eval_results = model.evaluate([X_test, A_test],\n",
        "                                    y_test,\n",
        "                                    batch_size=batch_size)\n",
        "\n",
        "      filename_val_loss = '/content/drive/My Drive/path_to_your_folder/gat_self_loop/loss/val_loss_' + 'M_' + str(M) + '_N_' + str(N) + '_K_' + str(K) + '.txt'\n",
        "      with open(filename_val_loss, 'wb') as f:\n",
        "          np.savetxt(f, np.array(estimator.history['val_loss']), delimiter=',')\n",
        "\n",
        "      filename_loss = '/content/drive/My Drive/path_to_your_folder/results/gat_self_loop/loss/loss_' + 'M_' + str(M) + '_N_' + str(N) + '_K_' + str(K) + '.txt'\n",
        "      with open(filename_loss, 'wb') as f:\n",
        "          np.savetxt(f, np.array(estimator.history['loss']), delimiter=',')\n",
        "\n",
        "      ################################################################################\n",
        "      # TEST MODEL\n",
        "      ################################################################################\n",
        "      \n",
        "      idx = 0\n",
        "\n",
        "      for snr in range(-30, 12, 2):\n",
        "        filename_test = '/content/drive/My Drive/path_to_your_folder/test_data/ch_est_data_' + 'M_' + str(M) + '_N_' + str(N) + '_K_10_snr_' + str(snr) + '.mat'\n",
        "        data = sio.loadmat(filename_test)\n",
        "\n",
        "        A = data[\"A\"]\n",
        "        X = data[\"X\"]\n",
        "        E = data[\"E\"]\n",
        "        A = np.ones(A.shape)\n",
        "        y = data[\"y\"]\n",
        "\n",
        "        # Parameters\n",
        "        P = X.shape[-2]       # Number of nodes in the graphs\n",
        "        F = X[0].shape[-1]    # Dimension of node features\n",
        "        S = E[0].shape[-1]    # Dimension of edge features\n",
        "        n_out = y.shape[-1]   # Dimension of the target\n",
        "\n",
        "        MM = int(M/2)\n",
        "\n",
        "        y_est = model.predict([X, A])\n",
        "        nmse_h = (np.sum((y_est[:, 0:2*M] - y[:, 0, 0:2*M])**2)/(len(y_est[:, 0:2*M])*y_est[:, 0:2*M].shape[-1]))/np.sum(y[:, 0, 0:2*M]**2)\n",
        "        nmse_g = (np.sum((y_est[:, 2*M:4*M] - y[:, 0, 2*M:4*M])**2)/(len(y_est[:, 2*M:4*M])*y_est[:, 2*M:4*M].shape[-1]))/np.sum(y[:, 0, 2*M:4*M]**2)\n",
        "        nmse_gh = (np.sum((y_est - y[:, 0, :])**2)/(len(y_est)*y_est.shape[-1]))/np.sum(y**2)\n",
        "        print(nmse_h, nmse_g, nmse_gh)\n",
        "\n",
        "        NMSE_h[iter, idx] = nmse_h\n",
        "        NMSE_g[iter, idx] = nmse_g\n",
        "        NMSE_gh[iter, idx] = nmse_gh\n",
        "        idx = idx + 1\n",
        "\n",
        "        h_est = y_est \n",
        "\n",
        "        filename_nmse_gh = '/content/drive/My Drive/path_to_your_folder/results/gat/channel_est/lower_N/channel_est_' + 'M_' + str(M) + '_N_' + str(N) + '_K_' + str(K) + '_snr_' + str(snr) + '_' + str(iter) + '.txt'\n",
        "        with open(filename_nmse_gh, 'wb') as f:\n",
        "          np.savetxt(f, h_est, delimiter=',')\n",
        "\n",
        "      filename_nmse_h = '/content/drive/My Drive/path_to_your_folder/results/gat/nmse_h_' + 'M_' + str(M) + '_N_' + str(N) + '_K_' + str(K) + '_snr_' + str(snr) + '.txt'\n",
        "      with open(filename_nmse_h, 'wb') as f:\n",
        "        np.savetxt(f, np.array(np.mean(NMSE_h, axis = 0)), delimiter=',')\n",
        "\n",
        "      filename_nmse_g = '/content/drive/My Drive/path_to_your_folder/results/gat/nmse_g_' + 'M_' + str(M) + '_N_' + str(N) + '_K_' + str(K) + '_snr_' + str(snr) + '.txt'\n",
        "      with open(filename_nmse_g, 'wb') as f:\n",
        "        np.savetxt(f, np.array(np.mean(NMSE_g, axis = 0)), delimiter=',')\n",
        "      \n",
        "      filename_nmse_gh = '/content/drive/My Drive/path_to_your_folder/results/gat/nmse_gh_' + 'M_' + str(M) + '_N_' + str(N) + '_K_' + str(K) + '_snr_' + str(snr) + '.txt'\n",
        "      with open(filename_nmse_gh, 'wb') as f:\n",
        "        np.savetxt(f, np.array(np.mean(NMSE_gh, axis = 0)), delimiter=',')\n",
        "\n",
        "      \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}